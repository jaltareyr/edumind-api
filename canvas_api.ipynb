{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ab92a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quiz meta: {'id': 557321, 'title': 'Quiz - output', 'html_url': 'https://instructure.charlotte.edu/courses/16676/quizzes/557321', 'mobile_url': 'https://instructure.charlotte.edu/courses/16676/quizzes/557321?force_user=1&persist_headless=1', 'description': '', 'quiz_type': 'assignment', 'time_limit': None, 'timer_autosubmit_disabled': False, 'shuffle_answers': False, 'show_correct_answers': True, 'scoring_policy': 'keep_highest', 'allowed_attempts': 1, 'one_question_at_a_time': False, 'question_count': 426, 'points_possible': 426.0, 'cant_go_back': False, 'access_code': None, 'ip_filter': None, 'due_at': None, 'lock_at': None, 'unlock_at': None, 'published': False, 'unpublishable': True, 'locked_for_user': True, 'lock_info': {'missing_permission': 'participate_as_student', 'asset_string': 'quizzes:quiz_557321'}, 'lock_explanation': 'This quiz is currently locked.', 'hide_results': None, 'show_correct_answers_at': None, 'hide_correct_answers_at': None, 'all_dates': [{'due_at': None, 'unlock_at': None, 'lock_at': None, 'title': 'Everyone', 'base': True}], 'can_unpublish': True, 'can_update': True, 'require_lockdown_browser': False, 'require_lockdown_browser_for_results': False, 'require_lockdown_browser_monitor': False, 'lockdown_browser_monitor_data': None, 'speed_grader_url': None, 'permissions': {'read': True, 'create': True, 'manage': True, 'update': True, 'submit': True, 'preview': True, 'delete': True, 'read_statistics': True, 'grade': True, 'review_grades': True, 'view_answer_audits': False, 'manage_assign_to': True}, 'quiz_reports_url': 'https://instructure.charlotte.edu/api/v1/courses/16676/quizzes/557321/reports', 'quiz_statistics_url': 'https://instructure.charlotte.edu/api/v1/courses/16676/quizzes/557321/statistics', 'message_students_url': 'https://instructure.charlotte.edu/api/v1/courses/16676/quizzes/557321/submission_users/message', 'section_count': 1, 'important_dates': False, 'quiz_submission_versions_html_url': 'https://instructure.charlotte.edu/courses/16676/quizzes/557321/submission_versions', 'assignment_id': 2476788, 'one_time_results': False, 'only_visible_to_overrides': False, 'visible_to_everyone': True, 'assignment_group_id': 5727, 'show_correct_answers_last_attempt': False, 'version_number': 2, 'has_access_code': False, 'post_to_sis': False, 'migration_id': '252249_i478669c7fa549970e36eac591cdca62e', 'in_paced_course': False, 'question_types': []}\n",
      "Classic /questions returned: 426\n",
      "✅ Exported 426 questions to quiz_557321_questions.csv\n"
     ]
    }
   ],
   "source": [
    "import os, time, io, zipfile, csv\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "BASE = os.environ[\"CANVAS_BASE_URL\"].rstrip(\"/\")\n",
    "TOKEN = os.environ[\"CANVAS_TOKEN\"]\n",
    "H = {\"Authorization\": f\"Bearer {TOKEN}\"}\n",
    "\n",
    "def _get(url, params=None):\n",
    "    r = requests.get(url, headers=H, params=params or {})\n",
    "    r.raise_for_status()\n",
    "    return r\n",
    "\n",
    "def _post(url, data=None, files=None):\n",
    "    r = requests.post(url, headers=H, data=data, files=files)\n",
    "    r.raise_for_status()\n",
    "    return r\n",
    "\n",
    "def quiz_meta(course_id, quiz_id):\n",
    "    url = f\"{BASE}/api/v1/courses/{course_id}/quizzes/{quiz_id}\"\n",
    "    return _get(url).json()\n",
    "\n",
    "def list_classic_questions(course_id, quiz_id):\n",
    "    url = f\"{BASE}/api/v1/courses/{course_id}/quizzes/{quiz_id}/questions\"\n",
    "    out, params = [], {\"per_page\": 100, \"include[]\": \"answers\"}\n",
    "    while url:\n",
    "        r = _get(url, params=params)\n",
    "        out.extend(r.json())\n",
    "        next_url = None\n",
    "        if \"link\" in r.headers:\n",
    "            for part in r.headers[\"link\"].split(\",\"):\n",
    "                seg_rel = part.split(\";\")\n",
    "                if any('rel=\"next\"' in s for s in seg_rel[1:]):\n",
    "                    next_url = seg_rel[0].strip()[1:-1]\n",
    "        url, params = next_url, {}\n",
    "    return out\n",
    "\n",
    "def start_qti_export(course_id):\n",
    "    url = f\"{BASE}/api/v1/courses/{course_id}/content_exports\"\n",
    "    # export all quizzes/content as QTI\n",
    "    r = _post(url, data={\"export_type\": \"qti\"})\n",
    "    return r.json()[\"id\"]\n",
    "\n",
    "def poll_export(course_id, export_id, timeout_s=600, poll_every=5):\n",
    "    url = f\"{BASE}/api/v1/courses/{course_id}/content_exports/{export_id}\"\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        j = _get(url).json()\n",
    "        status = j.get(\"workflow_state\") or j.get(\"progress\")  # workflow_state: 'queued'|'exporting'|'completed'|'failed'\n",
    "        if j.get(\"workflow_state\") == \"completed\" and j.get(\"attachment\"):\n",
    "            return j[\"attachment\"][\"url\"]\n",
    "        if j.get(\"workflow_state\") == \"failed\":\n",
    "            raise RuntimeError(f\"Export failed: {j}\")\n",
    "        if time.time() - start > timeout_s:\n",
    "            raise TimeoutError(\"QTI export polling timed out\")\n",
    "        time.sleep(poll_every)\n",
    "\n",
    "def download_bytes(url):\n",
    "    r = requests.get(url, headers=H)  # attachment URLs usually require auth\n",
    "    r.raise_for_status()\n",
    "    return r.content\n",
    "\n",
    "def _iter_qti_assessments(zipf: zipfile.ZipFile):\n",
    "    \"\"\"Yield (filename, xml_root) for each XML that looks like a QTI assessment.\"\"\"\n",
    "    for name in zipf.namelist():\n",
    "        if not name.lower().endswith(\".xml\"):\n",
    "            continue\n",
    "        try:\n",
    "            root = ET.fromstring(zipf.read(name))\n",
    "        except ET.ParseError:\n",
    "            continue\n",
    "        # QTI 1.2 “assessment” elements\n",
    "        if root.tag.endswith(\"assessment\") or root.find(\".//{*}assessment\") is not None:\n",
    "            # Normalize: root may be <questestinterop><assessment>...</assessment></questestinterop>\n",
    "            assess = root if root.tag.endswith(\"assessment\") else root.find(\".//{*}assessment\")\n",
    "            if assess is not None:\n",
    "                yield name, assess\n",
    "\n",
    "def _clean_text(x):\n",
    "    return (x or \"\").strip()\n",
    "\n",
    "def extract_items_from_assessment(assessment_elem):\n",
    "    \"\"\"Return list of items: id, title, stem (html/plain), and simple choice list if available.\"\"\"\n",
    "    items = []\n",
    "    for item in assessment_elem.findall(\".//{*}item\"):\n",
    "        ident = item.get(\"ident\")\n",
    "        title = item.get(\"title\")\n",
    "        # stem text (Canvas packs HTML in material/mattext)\n",
    "        mat = item.find(\".//{*}material/{*}mattext\")\n",
    "        stem = _clean_text(mat.text if mat is not None else \"\")\n",
    "        # choices (if present)\n",
    "        choices = []\n",
    "        for resp in item.findall(\".//{*}response_label\"):\n",
    "            lab_mat = resp.find(\".//{*}material/{*}mattext\")\n",
    "            txt = _clean_text(lab_mat.text if lab_mat is not None else \"\")\n",
    "            choices.append(txt)\n",
    "        items.append({\"id\": ident, \"title\": title, \"question_text\": stem, \"choices\": choices})\n",
    "    return items\n",
    "\n",
    "def find_assessment_for_quiz_title(zip_bytes, quiz_title):\n",
    "    with zipfile.ZipFile(io.BytesIO(zip_bytes)) as z:\n",
    "        candidates = []\n",
    "        for name, assess in _iter_qti_assessments(z):\n",
    "            a_title = assess.get(\"title\") or \"\"\n",
    "            if a_title.strip().lower() == quiz_title.strip().lower():\n",
    "                return name, assess\n",
    "            # fuzzy candidate if exact not found\n",
    "            if quiz_title.lower() in a_title.lower():\n",
    "                candidates.append((name, assess))\n",
    "        return (candidates[0] if candidates else (None, None))\n",
    "\n",
    "def write_csv(rows, output_file=\"quiz_questions.csv\"):\n",
    "    fieldnames = [\"id\", \"title\", \"question_text\", \"choices\"]\n",
    "    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            # flatten choices\n",
    "            r = r.copy()\n",
    "            r[\"choices\"] = \"; \".join(r.get(\"choices\", []))\n",
    "            w.writerow(r)\n",
    "    print(f\"✅ Exported {len(rows)} questions to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    course_id = 16676\n",
    "    quiz_id = 557321\n",
    "\n",
    "    qm = quiz_meta(course_id, quiz_id)\n",
    "    quiz_title = qm.get(\"title\") or f\"quiz_{quiz_id}\"\n",
    "    print(\"Quiz meta:\", qm)\n",
    "\n",
    "    classic = list_classic_questions(course_id, quiz_id)\n",
    "    print(f\"Classic /questions returned: {len(classic)}\")\n",
    "\n",
    "    if len(classic) > 0:\n",
    "        # You’ve got a Classic Quiz — export directly\n",
    "        rows = []\n",
    "        for q in classic:\n",
    "            answers = [a.get(\"text\", \"\") for a in q.get(\"answers\", [])]\n",
    "            rows.append({\n",
    "                \"id\": q.get(\"id\"),\n",
    "                \"title\": q.get(\"question_name\"),\n",
    "                \"question_text\": q.get(\"question_text\"),\n",
    "                \"choices\": answers\n",
    "            })\n",
    "        write_csv(rows, output_file=f\"quiz_{quiz_id}_questions.csv\")\n",
    "    else:\n",
    "        print(\"Looks like a New Quiz or bank-backed quiz. Falling back to QTI export…\")\n",
    "        export_id = start_qti_export(course_id)\n",
    "        url = poll_export(course_id, export_id)\n",
    "        zip_bytes = download_bytes(url)\n",
    "        name, assess = find_assessment_for_quiz_title(zip_bytes, quiz_title)\n",
    "        if not assess:\n",
    "            raise RuntimeError(\"Could not locate the assessment in the QTI export. Try checking the quiz title or inspect the ZIP manually.\")\n",
    "        items = extract_items_from_assessment(assess)\n",
    "        write_csv(items, output_file=f\"quiz_{quiz_id}_questions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4697e929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De-duplicated by 'id': 1284 -> 1284 rows\n",
      "Wrote cleaned file: quiz_merged_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# clean_quiz_csvs.py\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import html\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CONFIG ---\n",
    "INPUT_CSVS = [\n",
    "    \"quiz_436089_questions.csv\",\n",
    "    \"quiz_557321_questions.csv\",\n",
    "    \"quiz_557322_questions.csv\",\n",
    "    \"quiz_557324_questions.csv\"\n",
    "]\n",
    "\n",
    "OUTPUT_CSV = \"quiz_merged_clean.csv\"\n",
    "COLUMNS_TO_CLEAN = [\"question_text\"]  # add \"title\", \"choices\" if you want to clean those too\n",
    "DEDUP_BY_ID = True  # set False if you don't want deduplication\n",
    "ID_COLUMN = \"id\"\n",
    "\n",
    "# --- CLEANERS ---\n",
    "_ws_re = re.compile(r\"\\s+\")\n",
    "\n",
    "def clean_html_to_text(val: str) -> str:\n",
    "    if pd.isna(val):\n",
    "        return val\n",
    "    # Ensure it's a string\n",
    "    s = str(val)\n",
    "\n",
    "    # Parse with BeautifulSoup and drop all tags, including <script>/<style>\n",
    "    soup = BeautifulSoup(s, \"html.parser\")\n",
    "    # Remove script/style/noscript explicitly (sometimes get_text keeps their content out anyway,\n",
    "    # but this is extra safety for malformed HTML)\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = soup.get_text(separator=\" \", strip=True)\n",
    "    # Unescape HTML entities (&quot;, &amp;, etc.)\n",
    "    text = html.unescape(text)\n",
    "    # Normalize whitespace\n",
    "    text = _ws_re.sub(\" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def clean_df(df: pd.DataFrame, cols) -> pd.DataFrame:\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].apply(clean_html_to_text)\n",
    "    return df\n",
    "\n",
    "# --- MAIN ---\n",
    "def main():\n",
    "    frames = []\n",
    "    for path in INPUT_CSVS:\n",
    "        p = Path(path)\n",
    "        if not p.exists():\n",
    "            print(f\"Warning: file not found -> {p}\")\n",
    "            continue\n",
    "\n",
    "        # Use engine='python' to be extra tolerant of quotes/commas\n",
    "        df = pd.read_csv(\n",
    "            p,\n",
    "            dtype=str,               # keep everything as string to avoid dtype surprises\n",
    "            keep_default_na=False,   # keep empty strings as empty, not NaN\n",
    "            engine=\"python\"\n",
    "        )\n",
    "        df = clean_df(df, COLUMNS_TO_CLEAN)\n",
    "        frames.append(df)\n",
    "\n",
    "    if not frames:\n",
    "        print(\"No input files loaded. Please check INPUT_CSVS.\")\n",
    "        return\n",
    "\n",
    "    merged = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    if DEDUP_BY_ID and ID_COLUMN in merged.columns:\n",
    "        # Keep the first occurrence by id\n",
    "        before = len(merged)\n",
    "        merged = merged.drop_duplicates(subset=[ID_COLUMN], keep=\"first\").reset_index(drop=True)\n",
    "        print(f\"De-duplicated by '{ID_COLUMN}': {before} -> {len(merged)} rows\")\n",
    "\n",
    "    # Optional: consistent column order if your schema is fixed\n",
    "    preferred_order = [\"id\", \"title\", \"question_text\", \"choices\"]\n",
    "    cols = [c for c in preferred_order if c in merged.columns] + [c for c in merged.columns if c not in preferred_order]\n",
    "    merged = merged[cols]\n",
    "\n",
    "    merged.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"Wrote cleaned file: {OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e521994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensuring schema...\n",
      "\n",
      "[Action required once] Run this SQL in Supabase SQL editor (or set SUPABASE_DB_URL to let the script do it):\n",
      "\n",
      "create extension if not exists vector;\n",
      "\n",
      "create table if not exists quiz_questions (\n",
      "  id               bigint primary key,\n",
      "  title            text,\n",
      "  question_text    text,\n",
      "  choices          text,\n",
      "  question_name    text,\n",
      "  question_type    text,\n",
      "  points_possible  int,\n",
      "  answers          text\n",
      ");\n",
      "\n",
      "do $$\n",
      "begin\n",
      "  if not exists (\n",
      "    select 1\n",
      "    from information_schema.columns\n",
      "    where table_name = 'quiz_questions'\n",
      "      and column_name = 'embedding'\n",
      "  ) then\n",
      "    alter table quiz_questions add column embedding vector(768);\n",
      "  end if;\n",
      "end$$;\n",
      "-- Suggested later:\n",
      "-- create index if not exists quiz_questions_embedding_idx on quiz_questions using ivfflat (embedding vector_cosine_ops) with (lists = 100);\n",
      "\n",
      "Reading CSV(s)...\n",
      "Building texts from: joined fields\n",
      "Computing embeddings with BAAI/bge-base-en-v1.5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 21/21 [00:03<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing records...\n",
      "Upserting 1284 rows into quiz_questions...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# upload_quiz_to_supabase.py\n",
    "# ------------------------------------------------------------\n",
    "# pip install -q pandas python-dotenv requests sentence-transformers psycopg[binary]\n",
    "# ------------------------------------------------------------\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "CSV_PATHS = [\n",
    "    \"quiz_merged_clean.csv\"\n",
    "]\n",
    "TABLE_NAME = \"quiz_questions\"\n",
    "EMBED_COL = \"question_text\"\n",
    "MODEL_NAME = \"BAAI/bge-base-en-v1.5\"   # 768 dims\n",
    "BATCH_SIZE = 64                        # embedding batch size\n",
    "JOIN_COLUMNS_FOR_EMBED = True         # set True to embed concatenated fields\n",
    "JOIN_TEMPLATE = \"{question_text}\\nChoices: {choices}\"\n",
    "\n",
    "# ========== ENV ==========\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "SUPABASE_URL = os.environ.get(\"SUPABASE_URL\", \"\").rstrip(\"/\")\n",
    "SUPABASE_SERVICE_ROLE_KEY = os.environ.get(\"SUPABASE_SERVICE_ROLE_KEY\", \"\")\n",
    "SUPABASE_DB_URL = os.environ.get(\"SUPABASE_DB_URL\")  # optional for DDL\n",
    "\n",
    "if not SUPABASE_URL or not SUPABASE_SERVICE_ROLE_KEY:\n",
    "    sys.exit(\"Please set SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY env vars.\")\n",
    "\n",
    "REST_URL = f\"{SUPABASE_URL}/rest/v1/{TABLE_NAME}\"\n",
    "HEADERS_JSON = {\n",
    "    \"apikey\": SUPABASE_SERVICE_ROLE_KEY,\n",
    "    \"Authorization\": f\"Bearer {SUPABASE_SERVICE_ROLE_KEY}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "DDL_SQL = f\"\"\"\n",
    "create extension if not exists vector;\n",
    "\n",
    "create table if not exists {TABLE_NAME} (\n",
    "  id               bigint primary key,\n",
    "  title            text,\n",
    "  question_text    text,\n",
    "  choices          text,\n",
    "  question_name    text,\n",
    "  question_type    text,\n",
    "  points_possible  int,\n",
    "  answers          text\n",
    ");\n",
    "\n",
    "do $$\n",
    "begin\n",
    "  if not exists (\n",
    "    select 1\n",
    "    from information_schema.columns\n",
    "    where table_name = '{TABLE_NAME}'\n",
    "      and column_name = 'embedding'\n",
    "  ) then\n",
    "    alter table {TABLE_NAME} add column embedding vector(768);\n",
    "  end if;\n",
    "end$$;\n",
    "-- Suggested later:\n",
    "-- create index if not exists {TABLE_NAME}_embedding_idx on {TABLE_NAME} using ivfflat (embedding vector_cosine_ops) with (lists = 100);\n",
    "\"\"\"\n",
    "\n",
    "def ensure_schema_via_psycopg(sql: str) -> bool:\n",
    "    if not SUPABASE_DB_URL:\n",
    "        return False\n",
    "    try:\n",
    "        import psycopg\n",
    "        with psycopg.connect(SUPABASE_DB_URL) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(sql)\n",
    "        print(\"Schema ensured via direct Postgres connection.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] psycopg DDL failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def read_csvs(paths: List[str]) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for p in paths:\n",
    "        fp = Path(p)\n",
    "        if not fp.exists():\n",
    "            print(f\"[WARN] CSV not found: {fp}\")\n",
    "            continue\n",
    "        df = pd.read_csv(fp, dtype=str, keep_default_na=False, engine=\"python\")\n",
    "        frames.append(df)\n",
    "    if not frames:\n",
    "        sys.exit(\"No CSV files loaded.\")\n",
    "    return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "def build_text_for_embedding(row: Dict[str, Any]) -> str:\n",
    "    if JOIN_COLUMNS_FOR_EMBED:\n",
    "        return JOIN_TEMPLATE.format(\n",
    "            question_text=row.get(\"question_text\", \"\").strip(),\n",
    "            choices=row.get(\"choices\", \"\").strip(),\n",
    "        ).strip()\n",
    "    return str(row.get(EMBED_COL, \"\")).strip()\n",
    "\n",
    "def compute_embeddings(texts: List[str]) -> List[List[float]]:\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    vecs = model.encode(\n",
    "        texts,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    return [v.tolist() for v in vecs]\n",
    "\n",
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "def to_int_or_none(v):\n",
    "    s = str(v).strip()\n",
    "    return int(s) if s.isdigit() else None\n",
    "\n",
    "def prepare_records(df: pd.DataFrame, embeddings: List[List[float]]) -> List[Dict[str, Any]]:\n",
    "    out = []\n",
    "    for i, (_, row) in enumerate(df.iterrows()):\n",
    "        rec = {\n",
    "            \"id\": to_int_or_none(row.get(\"id\")),\n",
    "            \"title\": row.get(\"title\") or None,\n",
    "            \"question_text\": row.get(\"question_text\") or None,\n",
    "            \"choices\": row.get(\"choices\") or None,\n",
    "            \"question_name\": row.get(\"question_name\") or None,\n",
    "            \"question_type\": row.get(\"question_type\") or None,\n",
    "            \"points_possible\": to_int_or_none(row.get(\"points_possible\")),\n",
    "            \"answers\": row.get(\"answers\") or None,\n",
    "            \"embedding\": embeddings[i],\n",
    "        }\n",
    "        if rec[\"id\"] is not None:\n",
    "            out.append(rec)\n",
    "    return out\n",
    "\n",
    "def upsert_rows(rows: List[Dict[str, Any]]) -> None:\n",
    "    headers = HEADERS_JSON.copy()\n",
    "    headers[\"Prefer\"] = \"resolution=merge-duplicates\"\n",
    "    for batch in chunks(rows, 1000):\n",
    "        r = requests.post(REST_URL, headers=headers, json=batch, timeout=60)\n",
    "        if r.status_code not in (200, 201, 204):\n",
    "            raise RuntimeError(f\"Upsert error {r.status_code}: {r.text}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Ensuring schema...\")\n",
    "    created = ensure_schema_via_psycopg(DDL_SQL)\n",
    "    if not created:\n",
    "        print(\n",
    "            \"\\n[Action required once] Run this SQL in Supabase SQL editor (or set SUPABASE_DB_URL to let the script do it):\\n\"\n",
    "            + DDL_SQL\n",
    "        )\n",
    "\n",
    "    print(\"Reading CSV(s)...\")\n",
    "    df = read_csvs(CSV_PATHS)\n",
    "\n",
    "    # Ensure expected columns exist\n",
    "    for col in [\"id\",\"title\",\"question_text\",\"choices\",\"question_name\",\"question_type\",\"points_possible\",\"answers\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "\n",
    "    print(f\"Building texts from: {'joined fields' if JOIN_COLUMNS_FOR_EMBED else EMBED_COL}\")\n",
    "    texts = [build_text_for_embedding(r) for r in df.to_dict(orient=\"records\")]\n",
    "\n",
    "    print(f\"Computing embeddings with {MODEL_NAME}...\")\n",
    "    embeddings = compute_embeddings(texts)\n",
    "    if len(embeddings) != len(df):\n",
    "        sys.exit(\"Embedding count mismatch.\")\n",
    "\n",
    "    print(\"Preparing records...\")\n",
    "    records = prepare_records(df, embeddings)\n",
    "    if not records:\n",
    "        sys.exit(\"No valid records with 'id' to upsert.\")\n",
    "\n",
    "    print(f\"Upserting {len(records)} rows into {TABLE_NAME}...\")\n",
    "    upsert_rows(records)\n",
    "    print(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d0ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
